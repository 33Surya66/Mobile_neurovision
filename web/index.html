<!DOCTYPE html>
<html>
<head>
  <!--
    If you are serving your web app in a path other than the root, change the
    href value below to reflect the base path you are serving from.

    The path provided below has to start and end with a slash "/" in order for
    it to work correctly.

    For more details:
    * https://developer.mozilla.org/en-US/docs/Web/HTML/Element/base

    This is a placeholder for base href that will be replaced by the value of
    the `--base-href` argument provided to `flutter build`.
  -->
  <base href="$FLUTTER_BASE_HREF">

  <meta charset="UTF-8">
  <meta content="IE=Edge" http-equiv="X-UA-Compatible">
  <meta name="description" content="A new Flutter project.">

  <!-- iOS meta tags & icons -->
  <meta name="mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  <meta name="apple-mobile-web-app-title" content="mobile_neurovision">
  <link rel="apple-touch-icon" href="icons/Icon-192.png">

  <!-- Favicon -->
  <link rel="icon" type="image/png" href="favicon.png"/>

  <title>mobile_neurovision</title>
  <link rel="manifest" href="manifest.json">
    <!-- TensorFlow.js and Face Landmarks Detection (used for web facial keypoints).
      Use the combined tf.min.js to ensure `tf` global is registered and backends are available.
    -->
  <!-- Prefer MediaPipe Face Mesh for reliable landmarks in the browser -->
  <script src="https://cdn.jsdelivr.net/npm/@mediapipe/face_mesh@0.5/face_mesh.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/@mediapipe/camera_utils@0.5/camera_utils.js"></script>
  <!-- TFJS + face-landmarks-detection fallback (does not require MediaPipe runtime files) -->
  <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@3.21.0/dist/tf.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/@tensorflow-models/face-landmarks-detection@0.0.7/dist/face-landmarks-detection.min.js"></script>
</head>
<body>
  <script src="flutter_bootstrap.js" async></script>
    <!-- Container for NeuroVision web camera preview (inserted by JS).
         This is positioned by Flutter via JS interop so the <video> appears
         visually inside the Flutter placeholder card (quick CSS overlay approach).
    -->
  <div id="neurovision-webcam" style="position:absolute; left:0; top:0; width:0; height:0; display:none; z-index:9999; border-radius:18px; overflow:hidden; pointer-events:none;"></div>

  <script>
      (function(){
        // Combined face-detection + overlay positioning script.
        let _video = null;
        let _canvas = null;
        let _ctx = null;
        let _stream = null;
        let _detector = null;
        let _running = false;
  let _tfModel = null;
  let _faceMesh = null;

        // Helper to dynamically load a script and return a Promise.
        function _loadScriptAsync(src) {
          return new Promise((resolve, reject) => {
            try {
              const s = document.createElement('script');
              s.src = src;
              s.async = true;
              s.onload = () => resolve(s);
              s.onerror = (e) => reject(new Error('Script load error: ' + src));
              document.head.appendChild(s);
            } catch (e) {
              reject(e);
            }
          });
        }

        async function startFaceDetection() {
          const container = document.getElementById('neurovision-webcam') || document.createElement('div');
          // Ensure the overlay container is attached as the last child of <body>
          if (!document.getElementById('neurovision-webcam')) {
            container.id = 'neurovision-webcam';
            container.style.position = 'absolute';
            container.style.left = '0';
            container.style.top = '0';
            container.style.width = '0';
            container.style.height = '0';
            container.style.display = 'none';
            container.style.zIndex = '2147483647';
            container.style.borderRadius = '18px';
            container.style.overflow = 'hidden';
            container.style.pointerEvents = 'none';
            document.body.appendChild(container);
          } else {
            // Move existing container to end of body so it sits above Flutter canvas
            if (container.parentNode !== document.body) document.body.appendChild(container);
            // ensure it's topmost in stacking context
            container.style.zIndex = '2147483647';
            container.style.pointerEvents = 'none';
          }
          container.style.display = 'block';
          if (_running) return;

          _video = document.createElement('video');
          _video.autoplay = true;
          _video.playsInline = true;
          _video.style.width = '100%';
          _video.style.height = '100%';
          _video.style.objectFit = 'cover';
          _video.style.display = 'block';
            // Do not redeclare _tfModel/_faceMesh here — use the outer scoped variables

          _canvas = document.createElement('canvas');
          _canvas.style.position = 'absolute';
          _canvas.style.top = '0';
          _canvas.style.left = '0';
          // Make sure canvas doesn't intercept pointer events so Flutter can receive input
          _canvas.style.pointerEvents = 'none';
          // Ensure the overlay canvas is visually above the video and Flutter UI
          _canvas.style.zIndex = '2147483647';

          container.appendChild(_video);
              // mute to allow autoplay in some browsers
              _video.muted = true;
          container.appendChild(_canvas);
              _video.style.position = 'absolute';
              _video.style.top = '0';
              _video.style.left = '0';
              _video.style.width = '100%';
              _video.style.height = '100%';
              _video.style.objectFit = 'cover';
              _video.style.display = 'block';
              // video should sit behind the overlay canvas
              _video.style.zIndex = '2147483646';
            try {
              console.log('neurovision: video loadedmetadata', _video.videoWidth, _video.videoHeight);
              // initialize canvas CSS size to match container or video size
              const dpr = window.devicePixelRatio || 1;
              const logicalW = container.clientWidth || _video.videoWidth || 640;
              const logicalH = container.clientHeight || _video.videoHeight || 360;
              _canvas.style.background = 'transparent';
              _canvas.style.zIndex = '9999';
              const deviceW = Math.max(1, Math.floor(logicalW * dpr));
              const deviceH = Math.max(1, Math.floor(logicalH * dpr));
              _canvas.width = deviceW;
              _canvas.height = deviceH;
              _canvas.style.width = logicalW + 'px';
              _canvas.style.height = logicalH + 'px';
              _ctx = _canvas.getContext('2d');
              _ctx.setTransform(dpr, 0, 0, dpr, 0, 0);
              // draw a quick debug dot so users can confirm the canvas is visible
              try {
                console.log('neurovision: created video and canvas', logicalW, logicalH, deviceW, deviceH);
                _ctx.fillStyle = 'rgba(255,0,0,0.9)';
                _ctx.beginPath();
                _ctx.arc(logicalW / 2, logicalH / 2, 6, 0, Math.PI * 2);
                _ctx.fill();
              } catch (e) {
                /* ignore draw errors */
              }
            } catch (e) {
              console.warn('neurovision: loadedmetadata handler error', e);
            }

          try {
            _stream = await navigator.mediaDevices.getUserMedia({video: {facingMode:'user'}, audio:false});
            _video.srcObject = _stream;
            await _video.play();

            _canvas.width = container.clientWidth || _video.videoWidth || 640;
            _canvas.height = container.clientHeight || _video.videoHeight || 360;
            _ctx = _canvas.getContext('2d');

            // Try TF model first
                try {
                  if (globalThis.faceLandmarksDetection) {
                    try {
                      if (globalThis.tf && globalThis.tf.setBackend) {
                        await globalThis.tf.setBackend('webgl');
                        await globalThis.tf.ready();
                      }
                    } catch (error_) {
                      console.error('TF backend set/ready failed:', error_);
                    }

                    try {
                      // Prefer the TFJS package for live video so we don't depend on the
                      // MediaPipe runtime files being available from CDN in the client.
                      _tfModel = await globalThis.faceLandmarksDetection.load(globalThis.faceLandmarksDetection.SupportedPackages.tfjs);
                      console.log('TFJS face-landmarks model loaded for live video');
                      _running = true;
                      requestAnimationFrame(tick);
                      return;
                    } catch (error_) {
                      console.error('TFJS model load failed:', error_);
                      _tfModel = null;
                    }
                  }
            } catch (err) {
              console.error('TF init error (outer):', err);
              _tfModel = null;
            }

            // Try MediaPipe FaceMesh (reliable landmarks) before Shape Detection
            if (globalThis.FaceMesh) {
              try {
                _faceMesh = new globalThis.FaceMesh({locateFile: (file) => `https://cdn.jsdelivr.net/npm/@mediapipe/face_mesh/${file}`});
                _faceMesh.setOptions({maxNumFaces:1, refineLandmarks:true, minDetectionConfidence:0.5, minTrackingConfidence:0.5});
                _faceMesh.onResults((results) => {
                  // results.multiFaceLandmarks is an array of arrays of normalized landmarks
                  const dpr = window.devicePixelRatio || 1;
                  const logicalW = container.clientWidth || _video.videoWidth || 640;
                  const logicalH = container.clientHeight || _video.videoHeight || 360;

                  // ensure container is positioned so absolute canvas/video align
                  container.style.position = container.style.position || 'relative';

                  // make sure video sits behind canvas (use very high z-index values)
                  if (_video) _video.style.zIndex = '2147483646';
                  if (_canvas) _canvas.style.zIndex = '2147483647';

                  // Set canvas device size for crisp rendering, and CSS size to logical px
                  const deviceW = Math.max(1, Math.floor(logicalW * dpr));
                  const deviceH = Math.max(1, Math.floor(logicalH * dpr));
                  if (_canvas.width !== deviceW) _canvas.width = deviceW;
                  if (_canvas.height !== deviceH) _canvas.height = deviceH;
                  _canvas.style.width = logicalW + 'px';
                  _canvas.style.height = logicalH + 'px';

                  // Use transform to map drawing to logical pixels
                  _ctx.setTransform(dpr, 0, 0, dpr, 0, 0);
                  _ctx.clearRect(0, 0, logicalW, logicalH);

                  if (results.multiFaceLandmarks && results.multiFaceLandmarks.length) {

                    const landmarks = results.multiFaceLandmarks[0];
                    // debug: log landmark count
                    if (typeof console !== 'undefined') console.debug('neurovision: landmarks', landmarks.length);

                    // draw mesh connections (simple adjacent connections for visibility)
                    _ctx.strokeStyle = 'rgba(0,200,150,0.7)';
                    _ctx.lineWidth = 1.2;
                    _ctx.beginPath();
                    for (let i = 0; i < landmarks.length - 1; i++) {
                      const a = landmarks[i];
                      const b = landmarks[i + 1];
                      _ctx.moveTo(a.x * logicalW, a.y * logicalH);
                      _ctx.lineTo(b.x * logicalW, b.y * logicalH);
                    }
                    _ctx.stroke();

                    // draw points
                    _ctx.fillStyle = 'rgba(0,255,128,0.95)';
                    for (let i = 0; i < landmarks.length; i++) {
                      const lm = landmarks[i];
                      const x = lm.x * logicalW;
                      const y = lm.y * logicalH;
                      _ctx.beginPath();
                      _ctx.arc(x, y, 2, 0, Math.PI * 2);
                      _ctx.fill();
                    }

                    // Dispatch flattened numeric array [x0,y0,x1,y1,...] (normalized coords)
                    try {
                      const flat = new Array(landmarks.length * 2);
                      for (let i = 0; i < landmarks.length; i++) {
                        flat[i * 2] = landmarks[i].x;
                        flat[i * 2 + 1] = landmarks[i].y;
                      }
                      const evt = new CustomEvent('neurovision_landmarks', {detail: flat});
                      globalThis.dispatchEvent(evt);
                    } catch (e) {
                      console.error('landmark dispatch error', e);
                    }
                  }
                });
                console.log('MediaPipe FaceMesh initialized');
                _running = true;
                requestAnimationFrame(tick);
                return;
              } catch (error_) {
                console.error('FaceMesh init failed:', error_);
                _faceMesh = null;
              }
            }

            // Fallback to Shape Detection
            if ('FaceDetector' in globalThis) {
              try {
                _detector = new globalThis.FaceDetector({fastMode:true, maxDetectedFaces:5});
                _running = true;
                requestAnimationFrame(tick);
              } catch (err) {
                console.log('FaceDetector init error', err);
                _ctx.font = '18px Arial';
                _ctx.fillStyle = 'red';
                _ctx.fillText('FaceDetector init error: ' + err, 10, 30);
              }
            } else {
              _ctx.font = '16px Arial';
              _ctx.fillStyle = 'orange';
              _ctx.fillText('Face detection model not available in this browser.', 10, 30);
            }
          } catch (e) {
            console.log('Camera error', e);
            alert('Could not access camera: ' + e);
            container.style.display = 'none';
          }
        }

        async function tick() {
          if (!_running) return;
          try {
            // If MediaPipe FaceMesh is being used, feed the video frame into it
            if (_faceMesh) {
              try {
                await _faceMesh.send({image: _video});
              } catch (err) {
                console.error('FaceMesh send error', err);
              }
              requestAnimationFrame(tick);
              return;
            }

            _ctx.clearRect(0, 0, _canvas.width, _canvas.height);
            if (_tfModel) {
              const preds = await _tfModel.estimateFaces({input: _video, returnTensors: false, flipHorizontal: true, predictIrises: true});
              for (const pred of preds) {
                const mesh = pred.scaledMesh || pred.mesh || [];
                _ctx.fillStyle = 'rgba(0,255,128,0.9)';
                for (const pt of mesh) {
                  const [x, y] = pt;
                  _ctx.beginPath();
                  _ctx.arc(x, y, 1.4, 0, 2 * Math.PI);
                  _ctx.fill();
                }
                if (pred.boundingBox) {
                  const box = pred.boundingBox;
                  _ctx.strokeStyle = 'rgba(255,255,255,0.6)';
                  _ctx.lineWidth = 1;
                  _ctx.strokeRect(box.topLeft[0], box.topLeft[1], box.bottomRight[0] - box.topLeft[0], box.bottomRight[1] - box.topLeft[1]);
                }
              }
            } else if (_detector != null) {
              const faces = await _detector.detect(_video);
              for (const face of faces) {
                const box = face.boundingBox;
                _ctx.strokeStyle = 'lime';
                _ctx.lineWidth = 2;
                _ctx.strokeRect(box.x, box.y, box.width, box.height);
              }
            }
          } catch (error_) {
            console.error('Per-frame detection error:', error_);
          }
          requestAnimationFrame(tick);
        }

        function stopFaceDetection() {
          _running = false;
          if (_stream) {
            for (const t of _stream.getTracks()) t.stop();
            _stream = null;
          }
          if (_video && _video.parentNode) _video.remove();
          if (_canvas && _canvas.parentNode) _canvas.remove();
          _video = null; _canvas = null; _ctx = null; _detector = null; _tfModel = null;
        }

        // Overlay positioning helpers exposed on globalThis for Flutter to call
        function positionOverlay(x, y, w, h) {
          const c = document.getElementById('neurovision-webcam');
          if (!c) return;
          c.style.display = 'block';
          c.style.position = 'absolute';
          c.style.left = x + 'px';
          c.style.top = y + 'px';
          const logicalW = Math.max(0, w);
          const logicalH = Math.max(0, h);
          c.style.width = logicalW + 'px';
          c.style.height = logicalH + 'px';
          c.style.borderRadius = '18px';
          c.style.overflow = 'hidden';
          c.style.pointerEvents = 'none';
          const vid = c.querySelector('video');
          const cvs = c.querySelector('canvas');
          const dpr = window.devicePixelRatio || 1;
          if (vid) { vid.style.width = '100%'; vid.style.height = '100%'; vid.style.objectFit = 'cover'; }
          if (cvs) {
            // set canvas backing store to device pixels and CSS size to logical px
            cvs.width = Math.max(1, Math.floor(logicalW * dpr));
            cvs.height = Math.max(1, Math.floor(logicalH * dpr));
            cvs.style.width = logicalW + 'px';
            cvs.style.height = logicalH + 'px';
            try {
              const ctx = cvs.getContext('2d');
              ctx.setTransform(dpr, 0, 0, dpr, 0, 0);
            } catch (e) {}
          }
        }

        function hideOverlay() {
          const c = document.getElementById('neurovision-webcam');
          if (c) c.style.display = 'none';
        }

        // export to globalThis
        globalThis.startFaceDetection = startFaceDetection;
        globalThis.stopFaceDetection = stopFaceDetection;
        globalThis.positionOverlay = positionOverlay;
        globalThis.hideOverlay = hideOverlay;
        // Ensure overlay is topmost if Flutter or other elements create stacking contexts later
        globalThis.bringOverlayToFront = function() {
          const c = document.getElementById('neurovision-webcam');
          if (!c) return;
          if (c.parentNode !== document.body) document.body.appendChild(c);
          c.style.zIndex = '2147483647';
          const cvs = c.querySelector('canvas');
          const vid = c.querySelector('video');
          if (vid) vid.style.zIndex = '2147483646';
          if (cvs) cvs.style.zIndex = '2147483647';
        };

        // Process a single image provided as a data URL (returns Promise resolving
        // to flattened landmarks [x0,y0,x1,y1,...] or null).
        // Uses a shared MediaPipe FaceMesh instance (created on demand) and a
        // resolver queue so multiple calls won't overwrite handlers.
        globalThis.processImageDataUrl = function(dataUrl) {
          return new Promise((resolve) => {
            try {
              const img = new Image();
              img.crossOrigin = 'anonymous';
              img.src = dataUrl;
              img.onload = async () => {
                try {
                  // First try TFJS face-landmarks-detection (works without MediaPipe runtime)
                  if (globalThis.faceLandmarksDetection) {
                    try {
                      if (!globalThis._tfImageModel) {
                        console.log('neurovision: loading tfjs face-landmarks model for image');
                        globalThis._tfImageModel = await globalThis.faceLandmarksDetection.load(globalThis.faceLandmarksDetection.SupportedPackages.tfjs);
                      }
                      const preds = await globalThis._tfImageModel.estimateFaces({input: img, returnTensors: false, predictIrises: true});
                      if (preds && preds.length) {
                        const mesh = preds[0].scaledMesh || preds[0].mesh || preds[0];
                        const flat = new Array((mesh.length || 0) * 2);
                        for (let i = 0; i < mesh.length; i++) {
                          const p = mesh[i];
                          // mesh may be [x,y,z] in pixel coords — normalize by image size
                          const x = (p[0] || p.x || 0) / img.width;
                          const y = (p[1] || p.y || 0) / img.height;
                          flat[i * 2] = x;
                          flat[i * 2 + 1] = y;
                        }
                        resolve(flat);
                        return;
                      } else {
                        // no preds, fallthrough to other methods
                      }
                    } catch (tfErr) {
                      console.warn('neurovision: tfjs image model error', tfErr);
                    }
                  }

                  // If FaceMesh isn't present, try to load it dynamically
                  if (!globalThis.FaceMesh) {
                    console.log('neurovision: FaceMesh not present, attempting dynamic load');
                    try {
                      await _loadScriptAsync('https://cdn.jsdelivr.net/npm/@mediapipe/face_mesh@0.5/face_mesh.js');
                      // small delay to allow registration
                      await new Promise((r) => setTimeout(r, 50));
                      console.log('neurovision: dynamic FaceMesh load attempt finished; FaceMesh=', !!globalThis.FaceMesh);
                    } catch (errLoad) {
                      console.warn('neurovision: failed to load FaceMesh script dynamically', errLoad);
                      resolve(null);
                      return;
                    }
                  }

                  if (!globalThis.FaceMesh) {
                    console.warn('neurovision: FaceMesh still not available after dynamic load');
                    resolve(null);
                    return;
                  }

                  // initialize shared FaceMesh for image processing
                  if (!globalThis._imageFaceMesh) {
                    console.log('neurovision: creating shared image FaceMesh');
                    const fm = new globalThis.FaceMesh({locateFile: (file) => `https://cdn.jsdelivr.net/npm/@mediapipe/face_mesh/${file}`});
                    fm.setOptions({maxNumFaces:1, refineLandmarks:true, minDetectionConfidence:0.5, minTrackingConfidence:0.5});
                    fm._resolvers = [];
                    fm.onResults((results) => {
                      const resolvers = fm._resolvers.splice(0);
                      try {
                        if (results && results.multiFaceLandmarks && results.multiFaceLandmarks.length) {
                          const landmarks = results.multiFaceLandmarks[0];
                          const flat = new Array(landmarks.length * 2);
                          for (let i = 0; i < landmarks.length; i++) {
                            flat[i * 2] = landmarks[i].x;
                            flat[i * 2 + 1] = landmarks[i].y;
                          }
                          for (const r of resolvers) r(flat);
                        } else {
                          for (const r of resolvers) r(null);
                        }
                      } catch (e) {
                        for (const r of resolvers) r(null);
                      }
                    });
                    globalThis._imageFaceMesh = fm;
                  }

                  const fm = globalThis._imageFaceMesh;
                  // queue resolver; will be called by onResults
                  fm._resolvers.push(resolve);
                  try {
                    fm.send({image: img});
                  } catch (err) {
                    // immediately drain with null on error
                    const r = fm._resolvers.pop();
                    if (r) r(null);
                  }
                } catch (err) {
                  console.error('neurovision: processImageDataUrl inner error', err);
                  resolve(null);
                }
              };
              img.onerror = (e) => { console.warn('neurovision: image load error', e); resolve(null); };
            } catch (e) {
              console.error('neurovision: processImageDataUrl error', e);
              resolve(null);
            }
          });
        };
      })();
    </script>
  </body>
</html>
